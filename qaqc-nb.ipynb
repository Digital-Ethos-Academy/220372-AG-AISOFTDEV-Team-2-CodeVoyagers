{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed67a405",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4eaa9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the project's root directory to the Python path to ensure 'utils' can be imported.\n",
    "try:\n",
    "    project_root = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "except IndexError:\n",
    "    project_root = os.path.abspath(os.path.join(os.getcwd()))\n",
    "\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "from utils import setup_llm_client, get_completion, save_artifact, load_artifact, clean_llm_output\n",
    "\n",
    "client, model_name, api_provider = setup_llm_client(model_name=\"gemini-2.5-pro\")\n",
    "\n",
    "# Load the application code from Day 3 to provide context for test generation\n",
    "# Fix: Load directly from app/main.py instead of using load_artifact which looks in artifacts directory\n",
    "app_main_path = os.path.join(project_root, \"app\", \"main.py\")\n",
    "try:\n",
    "    with open(app_main_path, 'r', encoding='utf-8') as f:\n",
    "        app_code = f.read()\n",
    "    print(f\"Successfully loaded app code from: {app_main_path}\")\n",
    "except FileNotFoundError:\n",
    "    app_code = None\n",
    "    print(f\"Warning: Could not load app/main.py from {app_main_path}. Lab may not function correctly.\")\n",
    "except Exception as e:\n",
    "    app_code = None\n",
    "    print(f\"Error loading app/main.py: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3bca4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Write a prompt to generate happy path tests for your API.\n",
    "happy_path_tests_prompt = f\"\"\"\n",
    "REPLACE\n",
    "\"\"\"\n",
    "\n",
    "print(\"--- Generating Happy Path Tests ---\")\n",
    "if app_code:\n",
    "    generated_happy_path_tests = get_completion(happy_path_tests_prompt, client, model_name, api_provider)\n",
    "    cleaned_tests = clean_llm_output(generated_happy_path_tests, language='python')\n",
    "    print(cleaned_tests)\n",
    "    save_artifact(cleaned_tests, \"tests/test_main_simple.py\")\n",
    "else:\n",
    "    print(\"Skipping test generation because app code is missing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e0b5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Write a prompt to generate edge case tests.\n",
    "edge_case_tests_prompt = f\"\"\"\n",
    "REPLACE\n",
    "\"\"\"\n",
    "\n",
    "print(\"--- Generating Edge Case Tests ---\")\n",
    "if app_code:\n",
    "    generated_edge_case_tests = get_completion(edge_case_tests_prompt, client, model_name, api_provider)\n",
    "    cleaned_edge_case_tests = clean_llm_output(generated_edge_case_tests, language='python')\n",
    "    print(cleaned_edge_case_tests)\n",
    "else:\n",
    "    print(\"Skipping test generation because app code is missing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8273c24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Write a prompt to generate the pytest fixture for an isolated test database.\n",
    "db_fixture_prompt = f\"\"\"\n",
    "REPLACE\n",
    "\"\"\"\n",
    "\n",
    "print(\"--- Generating Pytest DB Fixture ---\")\n",
    "if app_code:\n",
    "    generated_db_fixture = get_completion(db_fixture_prompt, client, model_name, api_provider)\n",
    "    cleaned_fixture = clean_llm_output(generated_db_fixture, language='python')\n",
    "    print(cleaned_fixture)\n",
    "    save_artifact(cleaned_fixture, \"tests/conftest.py\", overwrite=True)\n",
    "else:\n",
    "    print(\"Skipping fixture generation because app context is missing.\")\n",
    "\n",
    "# TODO: Write a prompt to refactor the happy path tests to use the new fixture.\n",
    "refactor_tests_prompt = f\"\"\"\n",
    "You are a QA Engineer. Given the FastAPI application source below and the tests/conftest.py fixture that provides a pytest `client` fixture (TestClient configured to use an in-memory DB), generate a single Python file (no extra commentary) named tests/test_main_with_fixture.py.\n",
    "\n",
    "Requirements for tests/test_main_with_fixture.py:\n",
    "- Do NOT create a TestClient or import the app; rely on the `client` fixture (function parameter) provided by tests/conftest.py.\n",
    "- Import only what is necessary (e.g., time or datetime for generating unique emails).\n",
    "- Include two pytest test functions that use the `client` fixture:\n",
    "    1. test_create_user_happy_path(client):\n",
    "        - POST /users/ with a valid payload containing first_name, last_name, email, start_date.\n",
    "        - Assert response.status_code == 201\n",
    "        - Assert response.json() contains 'user_id', 'email', and 'full_name'\n",
    "        - Assert the returned 'email' matches the payload\n",
    "    2. test_list_users_happy_path(client):\n",
    "        - Ensure the test creates a user inside the test (POST /users/) to make the test independent.\n",
    "        - GET /users/ and assert response.status_code == 200\n",
    "        - Assert the response is a list and contains at least one user\n",
    "        - Assert the list contains the email created in this test\n",
    "- Use a timestamp-based unique email in each test so they remain independent even when run together.\n",
    "- Keep tests self-contained and runnable with pytest (include necessary imports).\n",
    "- Do not include explanations or non-code text â€” output only the Python source for tests/test_main_with_fixture.py.\n",
    "\n",
    "Application source for context (do not modify it; use to ensure correct field names and endpoints):\n",
    "{app_code}\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n--- Generating Refactored Tests ---\")\n",
    "if app_code:\n",
    "    refactored_tests = get_completion(refactor_tests_prompt, client, model_name, api_provider)\n",
    "    cleaned_refactored_tests = clean_llm_output(refactored_tests, language='python')\n",
    "    print(cleaned_refactored_tests)\n",
    "    save_artifact(cleaned_refactored_tests, \"tests/test_main_with_fixture.py\", overwrite=True)\n",
    "else:\n",
    "    print(\"Skipping test refactoring because app context is missing.\")\n",
    "\n",
    "print(\"\\n--- Generating Refactored Tests ---\")\n",
    "if app_code:\n",
    "    refactored_tests = get_completion(refactor_tests_prompt, client, model_name, api_provider)\n",
    "    cleaned_refactored_tests = clean_llm_output(refactored_tests, language='python')\n",
    "    print(cleaned_refactored_tests)\n",
    "    save_artifact(cleaned_refactored_tests, \"tests/test_main_with_fixture.py\", overwrite=True)\n",
    "else:\n",
    "    print(\"Skipping test refactoring because app context is missing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bffb4ca",
   "metadata": {},
   "source": [
    "**iNJECTING BUGGY CODE?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4cc65ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Task 1: Analyze and explain the bug --- \n",
    "analyze_bug_prompt = f\"\"\"\n",
    "You are a senior Python developer acting as a code reviewer.\n",
    "Analyze the following Python function and explain the logical error in it. Be concise.\n",
    "\n",
    "```python\n",
    "{buggy_code}\n",
    "```\n",
    "\"\"\"\n",
    "print(\"--- Analyzing Bug ---\")\n",
    "bug_explanation = get_completion(analyze_bug_prompt, client, model_name, api_provider)\n",
    "print(bug_explanation)\n",
    "\n",
    "# --- Task 2: Write a failing pytest test ---\n",
    "failing_test_prompt = f\"\"\"\n",
    "You are a QA engineer writing tests with pytest.\n",
    "Based on the following Python function, write a test function named `test_calculate_total_with_discount` that will FAIL because of the bug in the discount calculation. The test should assert the correct expected value.\n",
    "\n",
    "```python\n",
    "{buggy_code}\n",
    "```\n",
    "\"\"\"\n",
    "print(\"\\n--- Generating Failing Test ---\")\n",
    "failing_test_code_raw = get_completion(failing_test_prompt, client, model_name, api_provider)\n",
    "failing_test_code = clean_llm_output(failing_test_code_raw, language='python')\n",
    "print(failing_test_code)\n",
    "\n",
    "# --- Task 3: Fix the code based on the failing test ---\n",
    "fix_code_prompt = f\"\"\"\n",
    "You are a senior Python developer tasked with fixing a bug.\n",
    "\n",
    "The following function is buggy:\n",
    "<buggy_code>\n",
    "```python\n",
    "{buggy_code}\n",
    "```\n",
    "</buggy_code>\n",
    "\n",
    "The following pytest test fails when run against it, demonstrating the bug:\n",
    "<failing_test>\n",
    "```python\n",
    "{failing_test_code}\n",
    "```\n",
    "</failing_test>\n",
    "\n",
    "Your task is to fix the `calculate_total` function so that the provided test will pass. Output only the corrected function.\n",
    "\"\"\"\n",
    "print(\"\\n--- Generating Fixed Code ---\")\n",
    "fixed_code_raw = get_completion(fix_code_prompt, client, model_name, api_provider)\n",
    "fixed_code = clean_llm_output(fixed_code_raw, language='python')\n",
    "print(fixed_code)\n",
    "\n",
    "# Save the final corrected code\n",
    "if fixed_code:\n",
    "    save_artifact(fixed_code, \"app/day4_sp_fixed_shopping_cart.py\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
