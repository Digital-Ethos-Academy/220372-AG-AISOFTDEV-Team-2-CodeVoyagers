{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup\n",
    "\n",
    "**Explanation:**\n",
    "We load the `project_prd.md` artifact. This document is the single source of truth for our project's requirements and provides the essential context for the LLM to generate a relevant and accurate database schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: c:\\aiswe\\220372-AG-AISOFTDEV-Team-2-CodeVoyagers\n",
      "Project root directory: c:\\\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-06 07:35:41,033 ag_aisoftdev.utils INFO LLM Client configured provider=google model=gemini-2.5-pro latency_ms=None artifacts_path=None\n",
      "2025-11-06 07:35:42,334 ag_aisoftdev.utils INFO LLM Client configured provider=google model=gemini-2.5-pro latency_ms=None artifacts_path=None\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import sqlite3\n",
    "\n",
    "# Add the project's root directory to the Python path to ensure 'utils' can be imported.\n",
    "try:\n",
    "    project_root = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "except IndexError:\n",
    "    project_root = os.path.abspath(os.path.join(os.getcwd()))\n",
    "\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "print(f\"Current working directory: {os.getcwd()}\")\n",
    "print(f\"Project root directory: {project_root}\")\n",
    "\n",
    "from utils import setup_llm_client, get_completion, save_artifact, load_artifact, clean_llm_output, recommended_models_table, prompt_enhancer\n",
    "\n",
    "# Initialize separate LLM clients for different artifacts to use the latest models from different providers.\n",
    "# - Schema generation uses a strong instruction-following model\n",
    "# - Seed data generation uses a model tuned for data generation\n",
    "schema_client, schema_model_name, schema_api_provider = setup_llm_client(model_name=\"gemini-2.5-pro\")\n",
    "seed_client, seed_model_name, seed_api_provider = setup_llm_client(model_name=\"gemini-2.5-pro\")\n",
    "\n",
    "# Load the PRD\n",
    "prd_content = load_artifact(\"artifacts/project_prd.md\")\n",
    "if not prd_content:\n",
    "    print(\"Warning: Could not load prd_content = artifacts/project_prd.md. Lab may not function correctly.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 2 - Generating the SQL Schema (SQLite)\n",
    "\n",
    "**Explanation:**\n",
    "This prompt instructs the LLM to act as a Database Administrator (DBA) and generate a SQL schema specifically compatible with SQLite.\n",
    "\n",
    "Guidance for the LLM and the developer:\n",
    "- Output only `CREATE TABLE` statements and associated `CREATE INDEX` statements where helpful. Do not include any surrounding markdown fences or explanatory text in the SQL output.\n",
    "- Use SQLite-compatible types and conventions: prefer `INTEGER`, `TEXT`, `REAL`, `BLOB`, and `NUMERIC`. For auto-incrementing primary keys use `INTEGER PRIMARY KEY AUTOINCREMENT`. Do NOT use `SERIAL`, `BIGSERIAL`, `AUTO_INCREMENT`, or PostgreSQL/MySQL-specific types or DDL.\n",
    "- Avoid features not supported by SQLite such as `ALTER TABLE ... DROP COLUMN`, `CHECK` constraints that reference subqueries, or advanced index types. Keep DDL portable for SQLite's capabilities.\n",
    "- Use `FOREIGN KEY` clauses only where appropriate; remember that SQLite enforces foreign keys only when `PRAGMA foreign_keys = ON` is set by the application.\n",
    "- Provide sensible column constraints (`NOT NULL`, `UNIQUE`) and default values using SQLite-supported expressions.\n",
    "\n",
    "We will post-process the LLM response with `clean_llm_output(..., language='sql')` to strip markdown and save the pure SQL to `artifacts/schema.sql`. The notebook later uses `cursor.executescript()` to run the SQL, so ensure the output is a single SQL script containing multiple statements separated by semicolons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Generating SQL Schema ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-06 07:41:02,991 ag_aisoftdev.utils INFO LLM Client configured provider=openai model=o3 latency_ms=None artifacts_path=None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt enhancement failed ([openai:o3] prompt enhancement error: [openai:o3] completion error: Connection error.); falling back to original prompt.\n",
      "CREATE TABLE project_managers (\n",
      "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
      "    name TEXT NOT NULL,\n",
      "    role TEXT NOT NULL,\n",
      "    department TEXT,\n",
      "    email TEXT NOT NULL UNIQUE,\n",
      "    experience_years INTEGER NOT NULL,\n",
      "    focus_area TEXT,\n",
      "    active_project TEXT,\n",
      "    project_summary TEXT,\n",
      "    created_at TEXT DEFAULT (datetime('now'))\n",
      ");\n",
      "\n",
      "CREATE TABLE manager_required_skills (\n",
      "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
      "    manager_id INTEGER NOT NULL,\n",
      "    skill TEXT NOT NULL,\n",
      "    FOREIGN KEY (manager_id) REFERENCES project_managers(id) ON DELETE CASCADE,\n",
      "    UNIQUE(manager_id, skill)\n",
      ");\n",
      "\n",
      "CREATE TABLE employees (\n",
      "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
      "    manager_id INTEGER,\n",
      "    name TEXT NOT NULL,\n",
      "    title TEXT NOT NULL,\n",
      "    experience_years INTEGER NOT NULL,\n",
      "    education TEXT,\n",
      "    location TEXT,\n",
      "    summary TEXT,\n",
      "    created_at TEXT DEFAULT (datetime('now')),\n",
      "    FOREIGN KEY (manager_id) REFERENCES project_managers(id) ON DELETE SET NULL,\n",
      "    CHECK(experience_years >= 0)\n",
      ");\n",
      "\n",
      "CREATE TABLE employee_skills (\n",
      "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
      "    employee_id INTEGER NOT NULL,\n",
      "    skill TEXT NOT NULL,\n",
      "    FOREIGN KEY (employee_id) REFERENCES employees(id) ON DELETE CASCADE,\n",
      "    UNIQUE(employee_id, skill)\n",
      ");\n",
      "\n",
      "CREATE TABLE employee_metrics (\n",
      "    employee_id INTEGER PRIMARY KEY,\n",
      "    velocity INTEGER NOT NULL,\n",
      "    quality_score INTEGER NOT NULL,\n",
      "    projects_delivered INTEGER NOT NULL,\n",
      "    skill_alignment_score INTEGER NOT NULL,\n",
      "    FOREIGN KEY (employee_id) REFERENCES employees(id) ON DELETE CASCADE,\n",
      "    CHECK(quality_score BETWEEN 0 AND 100),\n",
      "    CHECK(skill_alignment_score BETWEEN 0 AND 100)\n",
      ");\n",
      "\n",
      "CREATE INDEX idx_project_managers_email ON project_managers(email);\n",
      "CREATE INDEX idx_employees_manager_id ON employees(manager_id);\n",
      "CREATE INDEX idx_employee_metrics_skill_alignment ON employee_metrics(skill_alignment_score);\n",
      "CREATE INDEX idx_manager_required_skills_skill ON manager_required_skills(skill);\n",
      "CREATE INDEX idx_employee_skills_skill ON employee_skills(skill);\n",
      "CREATE TABLE project_managers (\n",
      "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
      "    name TEXT NOT NULL,\n",
      "    role TEXT NOT NULL,\n",
      "    department TEXT,\n",
      "    email TEXT NOT NULL UNIQUE,\n",
      "    experience_years INTEGER NOT NULL,\n",
      "    focus_area TEXT,\n",
      "    active_project TEXT,\n",
      "    project_summary TEXT,\n",
      "    created_at TEXT DEFAULT (datetime('now'))\n",
      ");\n",
      "\n",
      "CREATE TABLE manager_required_skills (\n",
      "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
      "    manager_id INTEGER NOT NULL,\n",
      "    skill TEXT NOT NULL,\n",
      "    FOREIGN KEY (manager_id) REFERENCES project_managers(id) ON DELETE CASCADE,\n",
      "    UNIQUE(manager_id, skill)\n",
      ");\n",
      "\n",
      "CREATE TABLE employees (\n",
      "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
      "    manager_id INTEGER,\n",
      "    name TEXT NOT NULL,\n",
      "    title TEXT NOT NULL,\n",
      "    experience_years INTEGER NOT NULL,\n",
      "    education TEXT,\n",
      "    location TEXT,\n",
      "    summary TEXT,\n",
      "    created_at TEXT DEFAULT (datetime('now')),\n",
      "    FOREIGN KEY (manager_id) REFERENCES project_managers(id) ON DELETE SET NULL,\n",
      "    CHECK(experience_years >= 0)\n",
      ");\n",
      "\n",
      "CREATE TABLE employee_skills (\n",
      "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
      "    employee_id INTEGER NOT NULL,\n",
      "    skill TEXT NOT NULL,\n",
      "    FOREIGN KEY (employee_id) REFERENCES employees(id) ON DELETE CASCADE,\n",
      "    UNIQUE(employee_id, skill)\n",
      ");\n",
      "\n",
      "CREATE TABLE employee_metrics (\n",
      "    employee_id INTEGER PRIMARY KEY,\n",
      "    velocity INTEGER NOT NULL,\n",
      "    quality_score INTEGER NOT NULL,\n",
      "    projects_delivered INTEGER NOT NULL,\n",
      "    skill_alignment_score INTEGER NOT NULL,\n",
      "    FOREIGN KEY (employee_id) REFERENCES employees(id) ON DELETE CASCADE,\n",
      "    CHECK(quality_score BETWEEN 0 AND 100),\n",
      "    CHECK(skill_alignment_score BETWEEN 0 AND 100)\n",
      ");\n",
      "\n",
      "CREATE INDEX idx_project_managers_email ON project_managers(email);\n",
      "CREATE INDEX idx_employees_manager_id ON employees(manager_id);\n",
      "CREATE INDEX idx_employee_metrics_skill_alignment ON employee_metrics(skill_alignment_score);\n",
      "CREATE INDEX idx_manager_required_skills_skill ON manager_required_skills(skill);\n",
      "CREATE INDEX idx_employee_skills_skill ON employee_skills(skill);\n"
     ]
    }
   ],
   "source": [
    "schema_prompt = f\"\"\"\n",
    "You are a senior Database Administrator. Based on the provided PRD context below, generate a complete SQL \n",
    "schema that is fully compatible with SQLite.\n",
    "\n",
    "Requirements:\n",
    "- Output only valid SQLite DDL statements (e.g., CREATE TABLE, CREATE INDEX). Do NOT include any surrounding markdown, commentary, or explanation; output raw SQL only.\n",
    "- Use SQLite data types and conventions: INTEGER, TEXT, REAL, BLOB, NUMERIC. For auto-incrementing primary keys use `INTEGER PRIMARY KEY AUTOINCREMENT`.\n",
    "- Do NOT use PostgreSQL/MySQL-specific types or keywords such as SERIAL, BIGSERIAL, AUTO_INCREMENT, or `ENGINE=` options.\n",
    "- Avoid features unsupported by SQLite (e.g., ALTER TABLE ... DROP COLUMN, advanced index types). Keep the DDL runnable by SQLite's `sqlite3` and via Python's `cursor.executescript()`.\n",
    "- Include sensible NOT NULL, UNIQUE constraints and FOREIGN KEY clauses where appropriate. Note: application must enable foreign key enforcement via `PRAGMA foreign_keys = ON`.\n",
    "- Produce CREATE INDEX statements for columns frequently used in WHERE or JOIN clauses if helpful.\n",
    "- Ensure the output is a single SQL script with statements separated by semicolons.\n",
    "\n",
    "Data Model Entities (match application JSON fields):\n",
    "1. project_managers\n",
    "   Columns:\n",
    "     - id INTEGER PRIMARY KEY AUTOINCREMENT\n",
    "     - name TEXT NOT NULL\n",
    "     - role TEXT NOT NULL\n",
    "     - department TEXT\n",
    "     - email TEXT NOT NULL UNIQUE\n",
    "     - experience_years INTEGER NOT NULL\n",
    "     - focus_area TEXT\n",
    "     - active_project TEXT\n",
    "     - project_summary TEXT\n",
    "     - created_at TEXT DEFAULT (datetime('now'))\n",
    "\n",
    "2. manager_required_skills (one required skill per row)\n",
    "     - id INTEGER PRIMARY KEY AUTOINCREMENT\n",
    "     - manager_id INTEGER NOT NULL REFERENCES project_managers(id) ON DELETE CASCADE\n",
    "     - skill TEXT NOT NULL\n",
    "     - UNIQUE(manager_id, skill)\n",
    "\n",
    "3. employees\n",
    "     - id INTEGER PRIMARY KEY AUTOINCREMENT\n",
    "     - manager_id INTEGER REFERENCES project_managers(id) ON DELETE SET NULL\n",
    "     - name TEXT NOT NULL\n",
    "     - title TEXT NOT NULL\n",
    "     - experience_years INTEGER NOT NULL\n",
    "     - education TEXT\n",
    "     - location TEXT\n",
    "     - summary TEXT\n",
    "     - created_at TEXT DEFAULT (datetime('now'))\n",
    "     - CHECK(experience_years >= 0)\n",
    "\n",
    "4. employee_skills (one skill per row)\n",
    "     - id INTEGER PRIMARY KEY AUTOINCREMENT\n",
    "     - employee_id INTEGER NOT NULL REFERENCES employees(id) ON DELETE CASCADE\n",
    "     - skill TEXT NOT NULL\n",
    "     - UNIQUE(employee_id, skill)\n",
    "\n",
    "5. employee_metrics (exactly 4 numeric indicators attached 1:1 to employee)\n",
    "     - employee_id INTEGER PRIMARY KEY REFERENCES employees(id) ON DELETE CASCADE\n",
    "     - velocity INTEGER NOT NULL\n",
    "     - quality_score INTEGER NOT NULL\n",
    "     - projects_delivered INTEGER NOT NULL\n",
    "     - skill_alignment_score INTEGER NOT NULL\n",
    "     - CHECK(quality_score BETWEEN 0 AND 100)\n",
    "     - CHECK(skill_alignment_score BETWEEN 0 AND 100)\n",
    "\n",
    "Indexes:\n",
    "- Create indexes to optimize common lookups: project_managers(email), employees(manager_id), employee_metrics(skill_alignment_score), manager_required_skills(skill), employee_skills(skill).\n",
    "\n",
    "General Guidance:\n",
    "- Represent list fields (required_skills, skills) via their respective *_skills tables (one skill per row).\n",
    "- The metrics object must be represented by employee_metrics table with a 1:1 relationship to employees (employee_id as PRIMARY KEY + FOREIGN KEY).\n",
    "- Choose TEXT for timestamps (ISO 8601) using DEFAULT datetime('now').\n",
    "- Avoid triggers; keep schema straightforward.\n",
    "\n",
    "PRD CONTEXT:\n",
    "<prd>\n",
    "{prd_content}\n",
    "</prd>\n",
    "\n",
    "Now generate the SQLite-compatible SQL schema.\n",
    "\"\"\"\n",
    "\n",
    "print(\"--- Generating SQL Schema ---\")\n",
    "if prd_content:\n",
    "    try:\n",
    "        try:\n",
    "            enhanced_schema_prompt = prompt_enhancer(schema_prompt)\n",
    "            print(\"Schema Enhanced prompt\\n\", enhanced_schema_prompt)\n",
    "        except Exception as e:\n",
    "            print(f\"Prompt enhancement failed ({e}); falling back to original prompt.\")\n",
    "            enhanced_schema_prompt = schema_prompt\n",
    "\n",
    "        try:\n",
    "            generated_schema = get_completion(enhanced_schema_prompt, schema_client, schema_model_name, schema_api_provider)\n",
    "        except Exception as e:\n",
    "            print(f\"Schema generation failed ({e}). Aborting schema creation.\")\n",
    "            generated_schema = \"\"\n",
    "\n",
    "        cleaned_schema = clean_llm_output(generated_schema, language='sql') if generated_schema else \"\"\n",
    "        if cleaned_schema:\n",
    "            print(cleaned_schema)\n",
    "            save_artifact(cleaned_schema, \"artifacts/schema.sql\", overwrite=True)\n",
    "        else:\n",
    "            print(\"No schema produced.\")\n",
    "    finally:\n",
    "        # Ensure variable exists for downstream cells\n",
    "        cleaned_schema = cleaned_schema if 'cleaned_schema' in locals() else \"\"\n",
    "else:\n",
    "    print(\"Skipping schema generation because PRD is missing.\")\n",
    "    cleaned_schema = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 - Generating Realistic Seed Data\n",
    "\n",
    "**Explanation:**\n",
    "An empty database isn't very useful for development. In Step 2 we generated a SQLite schema and saved it to `artifacts/schema.sql` (and the cleaned SQL is available in the `cleaned_schema` variable).\n",
    "\n",
    "This step asks the LLM to produce realistic, referentially-consistent seed data as raw SQL `INSERT` statements that can be executed against that schema. The seed data generator MUST:\n",
    "\n",
    "- Inspect the provided SQL schema (the `<schema>` block below and `artifacts/schema.sql`) and use the exact table and column names from it.\n",
    "- Respect column types, `NOT NULL` and `UNIQUE` constraints, and any `FOREIGN KEY` relationships declared in the schema. If a primary key is defined as `INTEGER PRIMARY KEY AUTOINCREMENT`, the model may insert `NULL` for the PK and then reference the assigned PK values consistently for foreign keys, or insert explicit numeric IDs — but all foreign key references must remain valid within the generated script.\n",
    "- Wrap the generated inserts in a transaction (for example, `BEGIN; ... COMMIT;`) to ensure atomic seeding and easier rollback during development.\n",
    "- Output only raw SQL (no markdown fences or explanatory text).\n",
    "- Provide realistic, non-sensitive sample values (plausible names, emails, dates, statuses) that match the project's PRD context.\n",
    "\n",
    "The notebook will save the cleaned SQL to `artifacts/seed_data.sql` and then apply it to the database file in the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Generating Seed Data ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-06 07:42:40,931 ag_aisoftdev.utils INFO LLM Client configured provider=openai model=o3 latency_ms=None artifacts_path=None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed prompt enhancement failed ([openai:o3] prompt enhancement error: [openai:o3] completion error: Connection error.); using original prompt.\n",
      "BEGIN;\n",
      "\n",
      "-- Seed Project Managers\n",
      "INSERT INTO project_managers (id, name, role, department, email, experience_years, focus_area, active_project, project_summary) VALUES\n",
      "(1, 'Priya Patel', 'Project Manager', 'Engineering', 'priya.patel@insight.com', 8, 'AI/ML Initiatives', 'Project Phoenix', 'Developing a next-generation recommendation engine.'),\n",
      "(2, 'David Chen', 'Engineering Manager', 'Platform', 'david.chen@insight.com', 12, 'Core Infrastructure & Team Building', 'Project Chimera', 'Modernizing the core platform services for scalability.'),\n",
      "(3, 'Maria Garcia', 'Talent Mobility Specialist', 'HR', 'maria.garcia@insight.com', 6, 'Workforce Planning', 'Org Readiness Initiative', 'Assessing and mapping organizational skills for future growth.'),\n",
      "(4, 'John Smith', 'Senior Project Manager', 'Infrastructure', 'john.smith@insight.com', 15, 'Cloud Migration', 'Project Stratus', 'Migrating legacy on-premise services to a cloud-native architecture.'),\n",
      "(5, 'Emily White', 'Product Manager', 'Customer Experience', 'emily.white@insight.com', 7, 'Frontend Development', 'Project Horizon', 'Redesigning the customer-facing web application for improved usability.');\n",
      "\n",
      "-- Seed Manager Required Skills\n",
      "INSERT INTO manager_required_skills (manager_id, skill) VALUES\n",
      "(1, 'C++'),\n",
      "(1, 'Machine Learning'),\n",
      "(1, 'Python'),\n",
      "(2, 'Go'),\n",
      "(2, 'System Design'),\n",
      "(2, 'Leadership'),\n",
      "(3, 'Data Analysis'),\n",
      "(3, 'Workforce Planning'),\n",
      "(4, 'AWS'),\n",
      "(4, 'Kubernetes'),\n",
      "(4, 'Terraform'),\n",
      "(5, 'React'),\n",
      "(5, 'UX Design'),\n",
      "(5, 'Agile Methodology');\n",
      "\n",
      "-- Seed Employees\n",
      "INSERT INTO employees (id, manager_id, name, title, experience_years, education, location, summary) VALUES\n",
      "(101, 1, 'Anjali Sharma', 'Senior ML Engineer', 8, 'M.S. in Computer Science', 'San Francisco, CA', 'Expert in building and deploying large-scale machine learning models. Key contributor to Project Phoenix.'),\n",
      "(102, 1, 'Ben Carter', 'Software Engineer', 4, 'B.S. in Software Engineering', 'Austin, TX', 'Proficient C++ developer with a background in performance optimization.'),\n",
      "(103, 2, 'Carlos Diaz', 'Principal Engineer', 10, 'Ph.D. in Distributed Systems', 'New York, NY', 'Leads architecture for the Platform team, specializing in microservices and high-availability systems.'),\n",
      "(104, 2, 'Diana Ross', 'Mid-Level Engineer', 5, 'B.S. in Computer Science', 'Remote', 'Go developer focused on API development and containerization for Project Chimera.'),\n",
      "(105, 4, 'Ethan Hunt', 'DevOps Engineer', 6, 'B.S. in Information Technology', 'Seattle, WA', 'Manages CI/CD pipelines and Kubernetes clusters for the Infrastructure team.'),\n",
      "(106, 4, 'Fiona Glen', 'Cloud Architect', 9, 'M.S. in Cloud Computing', 'Boston, MA', 'Designs and implements cloud infrastructure solutions on AWS and Azure. Lead on Project Stratus.'),\n",
      "(107, 5, 'George Lee', 'Frontend Developer', 3, 'Coding Bootcamp Graduate', 'Chicago, IL', 'Specializes in building responsive and accessible user interfaces with React and TypeScript.'),\n",
      "(108, 5, 'Hannah Kim', 'UX/UI Designer', 5, 'B.A. in Graphic Design', 'San Francisco, CA', 'Leads user research and interface design for Project Horizon, ensuring a seamless user experience.'),\n",
      "(109, 2, 'Ivan Petrov', 'Senior Software Engineer', 7, 'M.S. in Computer Engineering', 'Remote', 'Experienced Go developer with a strong focus on backend services and database management.'),\n",
      "(110, 1, 'Jasmine Singh', 'Data Scientist', 4, 'M.S. in Data Science', 'New York, NY', 'Focuses on data analysis and feature engineering for the AI/ML team.');\n",
      "\n",
      "-- Seed Employee Skills\n",
      "INSERT INTO employee_skills (employee_id, skill) VALUES\n",
      "(101, 'C++'), (101, 'Machine Learning'), (101, 'Python'), (101, 'TensorFlow'),\n",
      "(102, 'C++'), (102, 'Python'), (102, 'SQL'), (102, 'Git'),\n",
      "(103, 'Go'), (103, 'System Design'), (103, 'PostgreSQL'), (103, 'Microservices'),\n",
      "(104, 'Go'), (104, 'Docker'), (104, 'gRPC'), (104, 'Kubernetes'),\n",
      "(105, 'AWS'), (105, 'Kubernetes'), (105, 'CI/CD'), (105, 'Bash Scripting'),\n",
      "(106, 'AWS'), (106, 'Terraform'), (106, 'Azure'), (106, 'Cloud-Native Technologies'),\n",
      "(107, 'React'), (107, 'TypeScript'), (107, 'CSS'), (107, 'JavaScript'),\n",
      "(108, 'Figma'), (108, 'UX Design'), (108, 'User Research'), (108, 'Agile Methodology'),\n",
      "(109, 'Go'), (109, 'System Design'), (109, 'SQL'), (109, 'Docker'),\n",
      "(110, 'Python'), (110, 'SQL'), (110, 'Data Analysis'), (110, 'Pandas');\n",
      "\n",
      "-- Seed Employee Metrics\n",
      "INSERT INTO employee_metrics (employee_id, velocity, quality_score, projects_delivered, skill_alignment_score) VALUES\n",
      "(101, 85, 98, 5, 95),\n",
      "(102, 70, 95, 8, 88),\n",
      "(103, 90, 99, 4, 97),\n",
      "(104, 75, 92, 6, 91),\n",
      "(105, 88, 96, 12, 94),\n",
      "(106, 82, 98, 3, 99),\n",
      "(107, 95, 90, 10, 85),\n",
      "(108, 80, 97, 7, 96),\n",
      "(109, 85, 94, 9, 93),\n",
      "(110, 78, 93, 11, 89);\n",
      "\n",
      "COMMIT;\n",
      "BEGIN;\n",
      "\n",
      "-- Seed Project Managers\n",
      "INSERT INTO project_managers (id, name, role, department, email, experience_years, focus_area, active_project, project_summary) VALUES\n",
      "(1, 'Priya Patel', 'Project Manager', 'Engineering', 'priya.patel@insight.com', 8, 'AI/ML Initiatives', 'Project Phoenix', 'Developing a next-generation recommendation engine.'),\n",
      "(2, 'David Chen', 'Engineering Manager', 'Platform', 'david.chen@insight.com', 12, 'Core Infrastructure & Team Building', 'Project Chimera', 'Modernizing the core platform services for scalability.'),\n",
      "(3, 'Maria Garcia', 'Talent Mobility Specialist', 'HR', 'maria.garcia@insight.com', 6, 'Workforce Planning', 'Org Readiness Initiative', 'Assessing and mapping organizational skills for future growth.'),\n",
      "(4, 'John Smith', 'Senior Project Manager', 'Infrastructure', 'john.smith@insight.com', 15, 'Cloud Migration', 'Project Stratus', 'Migrating legacy on-premise services to a cloud-native architecture.'),\n",
      "(5, 'Emily White', 'Product Manager', 'Customer Experience', 'emily.white@insight.com', 7, 'Frontend Development', 'Project Horizon', 'Redesigning the customer-facing web application for improved usability.');\n",
      "\n",
      "-- Seed Manager Required Skills\n",
      "INSERT INTO manager_required_skills (manager_id, skill) VALUES\n",
      "(1, 'C++'),\n",
      "(1, 'Machine Learning'),\n",
      "(1, 'Python'),\n",
      "(2, 'Go'),\n",
      "(2, 'System Design'),\n",
      "(2, 'Leadership'),\n",
      "(3, 'Data Analysis'),\n",
      "(3, 'Workforce Planning'),\n",
      "(4, 'AWS'),\n",
      "(4, 'Kubernetes'),\n",
      "(4, 'Terraform'),\n",
      "(5, 'React'),\n",
      "(5, 'UX Design'),\n",
      "(5, 'Agile Methodology');\n",
      "\n",
      "-- Seed Employees\n",
      "INSERT INTO employees (id, manager_id, name, title, experience_years, education, location, summary) VALUES\n",
      "(101, 1, 'Anjali Sharma', 'Senior ML Engineer', 8, 'M.S. in Computer Science', 'San Francisco, CA', 'Expert in building and deploying large-scale machine learning models. Key contributor to Project Phoenix.'),\n",
      "(102, 1, 'Ben Carter', 'Software Engineer', 4, 'B.S. in Software Engineering', 'Austin, TX', 'Proficient C++ developer with a background in performance optimization.'),\n",
      "(103, 2, 'Carlos Diaz', 'Principal Engineer', 10, 'Ph.D. in Distributed Systems', 'New York, NY', 'Leads architecture for the Platform team, specializing in microservices and high-availability systems.'),\n",
      "(104, 2, 'Diana Ross', 'Mid-Level Engineer', 5, 'B.S. in Computer Science', 'Remote', 'Go developer focused on API development and containerization for Project Chimera.'),\n",
      "(105, 4, 'Ethan Hunt', 'DevOps Engineer', 6, 'B.S. in Information Technology', 'Seattle, WA', 'Manages CI/CD pipelines and Kubernetes clusters for the Infrastructure team.'),\n",
      "(106, 4, 'Fiona Glen', 'Cloud Architect', 9, 'M.S. in Cloud Computing', 'Boston, MA', 'Designs and implements cloud infrastructure solutions on AWS and Azure. Lead on Project Stratus.'),\n",
      "(107, 5, 'George Lee', 'Frontend Developer', 3, 'Coding Bootcamp Graduate', 'Chicago, IL', 'Specializes in building responsive and accessible user interfaces with React and TypeScript.'),\n",
      "(108, 5, 'Hannah Kim', 'UX/UI Designer', 5, 'B.A. in Graphic Design', 'San Francisco, CA', 'Leads user research and interface design for Project Horizon, ensuring a seamless user experience.'),\n",
      "(109, 2, 'Ivan Petrov', 'Senior Software Engineer', 7, 'M.S. in Computer Engineering', 'Remote', 'Experienced Go developer with a strong focus on backend services and database management.'),\n",
      "(110, 1, 'Jasmine Singh', 'Data Scientist', 4, 'M.S. in Data Science', 'New York, NY', 'Focuses on data analysis and feature engineering for the AI/ML team.');\n",
      "\n",
      "-- Seed Employee Skills\n",
      "INSERT INTO employee_skills (employee_id, skill) VALUES\n",
      "(101, 'C++'), (101, 'Machine Learning'), (101, 'Python'), (101, 'TensorFlow'),\n",
      "(102, 'C++'), (102, 'Python'), (102, 'SQL'), (102, 'Git'),\n",
      "(103, 'Go'), (103, 'System Design'), (103, 'PostgreSQL'), (103, 'Microservices'),\n",
      "(104, 'Go'), (104, 'Docker'), (104, 'gRPC'), (104, 'Kubernetes'),\n",
      "(105, 'AWS'), (105, 'Kubernetes'), (105, 'CI/CD'), (105, 'Bash Scripting'),\n",
      "(106, 'AWS'), (106, 'Terraform'), (106, 'Azure'), (106, 'Cloud-Native Technologies'),\n",
      "(107, 'React'), (107, 'TypeScript'), (107, 'CSS'), (107, 'JavaScript'),\n",
      "(108, 'Figma'), (108, 'UX Design'), (108, 'User Research'), (108, 'Agile Methodology'),\n",
      "(109, 'Go'), (109, 'System Design'), (109, 'SQL'), (109, 'Docker'),\n",
      "(110, 'Python'), (110, 'SQL'), (110, 'Data Analysis'), (110, 'Pandas');\n",
      "\n",
      "-- Seed Employee Metrics\n",
      "INSERT INTO employee_metrics (employee_id, velocity, quality_score, projects_delivered, skill_alignment_score) VALUES\n",
      "(101, 85, 98, 5, 95),\n",
      "(102, 70, 95, 8, 88),\n",
      "(103, 90, 99, 4, 97),\n",
      "(104, 75, 92, 6, 91),\n",
      "(105, 88, 96, 12, 94),\n",
      "(106, 82, 98, 3, 99),\n",
      "(107, 95, 90, 10, 85),\n",
      "(108, 80, 97, 7, 96),\n",
      "(109, 85, 94, 9, 93),\n",
      "(110, 78, 93, 11, 89);\n",
      "\n",
      "COMMIT;\n"
     ]
    }
   ],
   "source": [
    "seed_data_prompt = f\"\"\"\n",
    "You are a data specialist. Based on the provided PRD and the exact SQL schema below, generate realistic SQL statements to seed the database for the onboarding tool.\n",
    "\n",
    "Requirements:\n",
    "- Inspect the provided SQL schema carefully (the `<schema>` block below and the file `artifacts/schema.sql`) and use the exact table and column names found there.\n",
    "- Produce a single SQL script that begins with `BEGIN;` and ends with `COMMIT;` to wrap all inserts in a transaction.\n",
    "- Ensure all `INSERT` statements respect column types, `NOT NULL` and `UNIQUE` constraints, and `FOREIGN KEY` relationships declared in the schema. If primary keys are `INTEGER PRIMARY KEY AUTOINCREMENT`, you may insert `NULL` for those columns and then reference the assigned values consistently for foreign keys, or insert explicit IDs — but relationships must be internally consistent.\n",
    "- Output only raw SQL (no markdown, no explanatory text).\n",
    "- Generate between 5 and 10 meaningful `INSERT` statements per major entity group (for example: users/project_managers/employees/applicants/onboarding_tasks) so the dataset is useful for development and testing.\n",
    "- Include at least 5 project managers and 3 employees, and a spread of onboarding tasks assigned to those users. Use realistic values for names, emails, dates (ISO 8601), and status fields consistent with the PRD.\n",
    "\n",
    "**PRD Context:**\n",
    "<prd>\n",
    "{prd_content}\n",
    "</prd>\n",
    "\n",
    "**SQL Schema:**\n",
    "<schema>\n",
    "{cleaned_schema}\n",
    "</schema>\n",
    "\"\"\"\n",
    "\n",
    "print(\"--- Generating Seed Data ---\")\n",
    "cleaned_seed_data = \"\"  # ensure variable exists even on failure\n",
    "if prd_content and cleaned_schema:\n",
    "    # Try to enhance the prompt; fall back silently on failure\n",
    "    try:\n",
    "        enhanced_seed_prompt = prompt_enhancer(seed_data_prompt)\n",
    "        print(\"Seed Data Enhanced prompt\\n\", enhanced_seed_prompt)\n",
    "    except Exception as e:\n",
    "        print(f\"Seed prompt enhancement failed ({e}); using original prompt.\")\n",
    "        enhanced_seed_prompt = seed_data_prompt\n",
    "\n",
    "    # Attempt seed data generation\n",
    "    try:\n",
    "        generated_seed_data = get_completion(enhanced_seed_prompt, seed_client, seed_model_name, seed_api_provider)\n",
    "    except Exception as e:\n",
    "        print(f\"Seed data generation failed ({e}).\")\n",
    "        generated_seed_data = \"\"\n",
    "\n",
    "    # Clean output only if something was generated\n",
    "    cleaned_seed_data = clean_llm_output(generated_seed_data, language='sql') if generated_seed_data else \"\"\n",
    "    if cleaned_seed_data:\n",
    "        print(cleaned_seed_data)\n",
    "        save_artifact(cleaned_seed_data, \"artifacts/seed_data.sql\", overwrite=True)\n",
    "    else:\n",
    "        print(\"No seed data produced.\")\n",
    "else:\n",
    "    print(\"Skipping seed data generation because PRD or schema is missing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4 - Creating and Seeding a Live Database\n",
    "\n",
    "**Explanation:**\n",
    "This Python function demonstrates a crucial engineering task: turning text-based artifacts into a live system component. The `create_database` function uses Python's built-in `sqlite3` library.\n",
    "1.  It establishes a connection to a database file, which creates the file if it doesn't exist.\n",
    "2.  It reads the `schema.sql` artifact and executes it. It's important to use `cursor.executescript()` here. While `cursor.execute()` is designed for a single SQL statement, `executescript()` is necessary for running a string that contains multiple SQL statements, which is exactly what our `schema.sql` and `seed_data.sql` files contain.\n",
    "3.  It then reads and executes the `seed_data.sql` artifact to populate the newly created tables.\n",
    "4.  `conn.commit()` saves all the changes to the database file.\n",
    "5.  The `finally` block ensures that `conn.close()` is always called, which is a critical best practice to prevent resource leaks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully connected to database at c:\\aiswe\\220372-AG-AISOFTDEV-Team-2-CodeVoyagers\\artifacts\\main_database.db\n",
      "Tables created successfully.\n",
      "Seed data inserted successfully.\n",
      "Database changes committed.\n",
      "Tables created successfully.\n",
      "Seed data inserted successfully.\n",
      "Database changes committed.\n"
     ]
    }
   ],
   "source": [
    "def create_database(db_path, schema_path, seed_path):\n",
    "    \"\"\"Creates and seeds a SQLite database from SQL files.\"\"\"\n",
    "    if not os.path.exists(schema_path):\n",
    "        print(f\"Error: Schema file not found at {schema_path}\")\n",
    "        return\n",
    "\n",
    "    # Delete the old database file if it exists to start fresh\n",
    "    if os.path.exists(db_path):\n",
    "        os.remove(db_path)\n",
    "        print(f\"Removed existing database file at {db_path}\")\n",
    "\n",
    "    conn = None\n",
    "    try:\n",
    "        conn = sqlite3.connect(db_path)\n",
    "        # Enable foreign key enforcement for this connection\n",
    "        conn.execute(\"PRAGMA foreign_keys = ON\")\n",
    "        cursor = conn.cursor()\n",
    "        print(f\"Successfully connected to database at {db_path}\")\n",
    "\n",
    "        # Read and execute the schema file\n",
    "        schema_sql = load_artifact(schema_path)\n",
    "        if schema_sql:\n",
    "            cursor.executescript(schema_sql)\n",
    "            print(\"Tables created successfully.\")\n",
    "\n",
    "        # Read and execute the seed data file if it exists\n",
    "        if os.path.exists(seed_path):\n",
    "            seed_sql = load_artifact(seed_path)\n",
    "            if seed_sql:\n",
    "                cursor.executescript(seed_sql)\n",
    "                print(\"Seed data inserted successfully.\")\n",
    "\n",
    "        conn.commit()\n",
    "        print(\"Database changes committed.\")\n",
    "\n",
    "    except sqlite3.Error as e:\n",
    "        print(f\"Database error: {e}\")\n",
    "    finally:\n",
    "        if conn:\n",
    "            conn.close()\n",
    "\n",
    "# Define file paths\n",
    "db_file = os.path.join(os.getcwd(), \"artifacts\", \"main_database.db\")\n",
    "schema_file = os.path.join(os.getcwd(), \"artifacts\", \"schema.sql\")\n",
    "seed_file = os.path.join(os.getcwd(), \"artifacts\", \"seed_data.sql\")\n",
    "\n",
    "# Execute the function to create and seed the database\n",
    "create_database(db_file, schema_file, seed_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5 - Verify the database was created successfully by querying the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to database at c:\\aiswe\\220372-AG-AISOFTDEV-Team-2-CodeVoyagers\\artifacts\\main_database.db\n",
      "Table 'project_managers' contains 5 records\n",
      "--- Sample rows from project_managers ---\n",
      "{'id': 1, 'name': 'Priya Patel', 'role': 'Project Manager', 'department': 'Engineering'}\n",
      "{'id': 2, 'name': 'David Chen', 'role': 'Engineering Manager', 'department': 'Platform'}\n",
      "{'id': 3, 'name': 'Maria Garcia', 'role': 'Talent Mobility Specialist', 'department': 'HR'}\n",
      "Table 'manager_required_skills' contains 14 records\n",
      "--- Sample rows from manager_required_skills ---\n",
      "{'id': 1, 'manager_id': 1, 'skill': 'C++'}\n",
      "{'id': 2, 'manager_id': 1, 'skill': 'Machine Learning'}\n",
      "{'id': 3, 'manager_id': 1, 'skill': 'Python'}\n",
      "Table 'employees' contains 10 records\n",
      "--- Sample rows from employees ---\n",
      "{'id': 101, 'manager_id': 1, 'name': 'Anjali Sharma', 'title': 'Senior ML Engineer'}\n",
      "{'id': 102, 'manager_id': 1, 'name': 'Ben Carter', 'title': 'Software Engineer'}\n",
      "{'id': 103, 'manager_id': 2, 'name': 'Carlos Diaz', 'title': 'Principal Engineer'}\n",
      "Table 'employee_skills' contains 40 records\n",
      "--- Sample rows from employee_skills ---\n",
      "{'id': 1, 'employee_id': 101, 'skill': 'C++'}\n",
      "{'id': 2, 'employee_id': 101, 'skill': 'Machine Learning'}\n",
      "{'id': 3, 'employee_id': 101, 'skill': 'Python'}\n",
      "Table 'employee_metrics' contains 10 records\n",
      "--- Sample rows from employee_metrics ---\n",
      "{'employee_id': 101, 'velocity': 85, 'quality_score': 98, 'projects_delivered': 5}\n",
      "{'employee_id': 102, 'velocity': 70, 'quality_score': 95, 'projects_delivered': 8}\n",
      "{'employee_id': 103, 'velocity': 90, 'quality_score': 99, 'projects_delivered': 4}\n"
     ]
    }
   ],
   "source": [
    "# Verify the database was created successfully by querying the data\n",
    "def verify_database(db_path, schema_path=None):\n",
    "    \"\"\"Verify the database contains the expected data based on the generated schema.\"\"\"\n",
    "    if not os.path.exists(db_path):\n",
    "        print(f\"Database file not found at {db_path}\")\n",
    "        return\n",
    "\n",
    "    conn = None\n",
    "    try:\n",
    "        conn = sqlite3.connect(db_path)\n",
    "        # Ensure foreign keys are enforced for verification queries\n",
    "        conn.execute(\"PRAGMA foreign_keys = ON\")\n",
    "        cursor = conn.cursor()\n",
    "        print(f\"Connected to database at {db_path}\")\n",
    "\n",
    "        # Determine expected tables from the provided schema if available\n",
    "        schema_sql = \"\"\n",
    "        if schema_path and os.path.exists(schema_path):\n",
    "            schema_sql = load_artifact(schema_path) or \"\"\n",
    "        elif 'cleaned_schema' in globals() and cleaned_schema:\n",
    "            schema_sql = cleaned_schema\n",
    "\n",
    "        table_names = []\n",
    "        if schema_sql:\n",
    "            import re\n",
    "            # Extract table names from CREATE TABLE statements\n",
    "            matches = re.findall(r'CREATE\\s+TABLE\\s+(?:IF\\s+NOT\\s+EXISTS\\s+)?[\"`]?(\\w+)[\"`]?', schema_sql, flags=re.IGNORECASE)\n",
    "            table_names = list(dict.fromkeys(matches))  # preserve order, remove duplicates\n",
    "\n",
    "        # Fallback to common expected tables if schema parsing found nothing\n",
    "        if not table_names:\n",
    "            table_names = ['users', 'applicants', 'onboarding_tasks']\n",
    "\n",
    "        # Query each table for counts and a small sample of rows\n",
    "        for t in table_names:\n",
    "            try:\n",
    "                cursor.execute(f\"SELECT COUNT(*) FROM {t}\")\n",
    "                count = cursor.fetchone()[0]\n",
    "                print(f\"Table '{t}' contains {count} records\")\n",
    "\n",
    "                # Get column names to present a representative sample row as a dict\n",
    "                cursor.execute(f\"PRAGMA table_info({t})\")\n",
    "                cols = [r[1] for r in cursor.fetchall()]\n",
    "                if cols:\n",
    "                    sel_cols = cols[:4]  # limit number of displayed columns\n",
    "                    cursor.execute(f\"SELECT {', '.join(sel_cols)} FROM {t} LIMIT 3\")\n",
    "                    rows = cursor.fetchall()\n",
    "                    if rows:\n",
    "                        print(f\"--- Sample rows from {t} ---\")\n",
    "                        for row in rows:\n",
    "                            # Align values with column names for readability\n",
    "                            sample = dict(zip(sel_cols, row))\n",
    "                            print(sample)\n",
    "            except sqlite3.Error as e:\n",
    "                print(f\"Could not query table {t}: {e}\")\n",
    "\n",
    "    except sqlite3.Error as e:\n",
    "        print(f\"Database error: {e}\")\n",
    "    finally:\n",
    "        if conn:\n",
    "            conn.close()\n",
    "\n",
    "# Verify the database using the generated schema when available\n",
    "verify_database(db_file, schema_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
